{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a5c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading runTrack...\n",
      "Loading config...\n",
      "Loading utils...\n",
      "Loading hdf5Track...\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "from os import path\n",
    "from common.utils import mini_batch_train\n",
    "from ddpg.ddpg import DDPGAgent\n",
    "from TRACK_env import TRACKenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b1d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #initialize environment\n",
    "# env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "# max_steps = 100 #max number of steps per episode\n",
    "# batch_size = 300 #batch size for updates\n",
    "# buffer_maxlen = 100000 #max buffer size\n",
    "\n",
    "# # define training hyperparameters\n",
    "# gamma = 0.99 #discount factor\n",
    "# tau = 1e-2  #for updates with target network\n",
    "\n",
    "# critic_lr = 1e-3 # learning rate\n",
    "# actor_lr = 1e-3 # learning rate\n",
    "\n",
    "# #initialize agent\n",
    "# agent = DDPGAgent(env, gamma, tau, buffer_maxlen, critic_lr, actor_lr, max_action = 2) #max_action is set by what the environment expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72830f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_episodes = 200\n",
    "# episode_rewards = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ef26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #initialize environment\n",
    "# env = gym.make('LunarLanderContinuous-v2')\n",
    "\n",
    "# max_steps = 100 #max number of steps per episode\n",
    "# batch_size = 300 #batch size for updates\n",
    "# buffer_maxlen = 100000 #max buffer size\n",
    "\n",
    "# # define training hyperparameters\n",
    "# gamma = 0.99 #discount factor\n",
    "# tau = 1e-2  #for updates with target network\n",
    "\n",
    "# critic_lr = 1e-4 # learning rate\n",
    "# actor_lr = 1e-4 # learning rate\n",
    "\n",
    "# #initialize agent\n",
    "# agent = DDPGAgent(env, gamma, tau, buffer_maxlen, critic_lr, actor_lr, max_action = 2) #max_action is set by what the environment expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6efa5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_episodes = 350\n",
    "# episode_rewards = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab169c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\trana\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\cuda\\__init__.py:80: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#initialize environment\n",
    "env = TRACKenv()\n",
    "\n",
    "max_steps = 100 #max number of steps per episode\n",
    "batch_size = 300 #batch size for updates\n",
    "buffer_maxlen = 100000 #max buffer size\n",
    "\n",
    "# define training hyperparameters\n",
    "gamma = 0.99 #discount factor\n",
    "tau = 1e-2  #for updates with target network\n",
    "\n",
    "critic_lr = 1e-4 # learning rate\n",
    "actor_lr = 1e-4 # learning rate\n",
    "\n",
    "#initialize agent\n",
    "agent = DDPGAgent(env, gamma, tau, buffer_maxlen, critic_lr, actor_lr, max_action = 2) #max_action is set by what the environment expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step reward: 236.0 ,Quadrupole vector: [ 1.3092965  3.1457465  1.5012741 -1.5706677  3.6938205  3.6404912]\n",
      "Step reward: 178.0 ,Quadrupole vector: [ 0.7379638  3.2424536  2.3266044 -1.767467   4.3681545  4.269753 ]\n",
      "Step reward: 150.0 ,Quadrupole vector: [ 0.09685761  3.3479111   3.2193394  -1.9874978   5.1113677   4.9542084 ]\n",
      "Step reward: 136.0 ,Quadrupole vector: [-0.6133484  3.4451466  4.1678343 -2.205866   5.9198246  5.6821375]\n",
      "Step reward: 74.0 ,Quadrupole vector: [-1.3907084  3.502627   5.169326  -2.3977344  6.781191   6.449262 ]\n",
      "Step reward: 49.0 ,Quadrupole vector: [-2.248463   3.529018   6.214519  -2.5420232  7.690727   7.262697 ]\n",
      "Step reward: 25.0 ,Quadrupole vector: [-3.1858354  3.5266488  7.3075294 -2.6474824  8.         8.       ]\n",
      "Step reward: 14.0 ,Quadrupole vector: [-4.1720824  3.4738026  8.        -2.7161238  8.         8.       ]\n",
      "Step reward: 20.0 ,Quadrupole vector: [-5.180157   3.361613   8.        -2.7873611  8.         8.       ]\n",
      "Step reward: 38.0 ,Quadrupole vector: [-6.2217417  3.2828288  8.        -2.8485637  8.         8.       ]\n",
      "Step reward: 38.0 ,Quadrupole vector: [-7.3101296  3.2765     8.        -2.888982   8.         8.       ]\n",
      "Step reward: 28.0 ,Quadrupole vector: [-8.         3.3382552  8.        -2.9033198  8.         8.       ]\n",
      "Step reward: 20.0 ,Quadrupole vector: [-8.        3.421875  8.       -2.905189  8.        8.      ]\n",
      "Step reward: 19.0 ,Quadrupole vector: [-8.         3.498702   8.        -2.9050941  8.         8.       ]\n",
      "Step reward: 22.0 ,Quadrupole vector: [-8.         3.5692315  8.        -2.9031756  8.         8.       ]\n",
      "Step reward: 21.0 ,Quadrupole vector: [-8.         3.6341002  8.        -2.8992262  8.         8.       ]\n",
      "Step reward: 20.0 ,Quadrupole vector: [-8.        3.694006  8.       -2.892796  8.        8.      ]\n",
      "Step reward: 26.0 ,Quadrupole vector: [-8.         3.7492256  8.        -2.884079   8.         8.       ]\n",
      "Step reward: 30.0 ,Quadrupole vector: [-8.        3.800022  8.       -2.873258  8.        8.      ]\n",
      "Step reward: 31.0 ,Quadrupole vector: [-8.         3.8465574  8.        -2.86052    8.         8.       ]\n",
      "Step reward: 29.0 ,Quadrupole vector: [-8.         3.888169   8.        -2.8461707  8.         8.       ]\n",
      "Step reward: 29.0 ,Quadrupole vector: [-8.         3.9252348  8.        -2.8303907  8.         8.       ]\n",
      "Step reward: 27.0 ,Quadrupole vector: [-8.         3.9581058  8.        -2.813346   8.         8.       ]\n",
      "Step reward: 27.0 ,Quadrupole vector: [-8.         3.9871092  8.        -2.7951896  8.         8.       ]\n",
      "Step reward: 28.0 ,Quadrupole vector: [-8.         4.0125494  8.        -2.7760627  8.         8.       ]\n",
      "Step reward: 32.0 ,Quadrupole vector: [-8.        4.034709  8.       -2.756095  8.        8.      ]\n",
      "Step reward: 29.0 ,Quadrupole vector: [-8.         4.053851   8.        -2.7354057  8.         8.       ]\n",
      "Step reward: 28.0 ,Quadrupole vector: [-8.         4.0702205  8.        -2.7141042  8.         8.       ]\n",
      "Step reward: 28.0 ,Quadrupole vector: [-8.         4.084044   8.        -2.6922913  8.         8.       ]\n",
      "Step reward: 29.0 ,Quadrupole vector: [-8.         4.095533   8.        -2.6700587  8.         8.       ]\n",
      "Step reward: 29.0 ,Quadrupole vector: [-8.        4.104883  8.       -2.647491  8.        8.      ]\n",
      "Step reward: 29.0 ,Quadrupole vector: [-8.         4.1122766  8.        -2.6246653  8.         8.       ]\n",
      "Step reward: 32.0 ,Quadrupole vector: [-8.         4.117882   8.        -2.6016517  8.         8.       ]\n",
      "Step reward: 30.0 ,Quadrupole vector: [-8.         4.1218557  8.        -2.5785146  8.         8.       ]\n",
      "Step reward: 29.0 ,Quadrupole vector: [-8.         4.124343   8.        -2.5553122  8.         8.       ]\n",
      "Step reward: 30.0 ,Quadrupole vector: [-8.         4.1254783  8.        -2.5320973  8.         8.       ]\n",
      "Step reward: 31.0 ,Quadrupole vector: [-8.         4.1253867  8.        -2.5089183  8.         8.       ]\n"
     ]
    }
   ],
   "source": [
    "max_episodes = 20\n",
    "episode_rewards = mini_batch_train(env, agent, max_episodes, max_steps, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
