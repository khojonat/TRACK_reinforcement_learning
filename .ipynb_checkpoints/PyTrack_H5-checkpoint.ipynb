{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7863f7d3",
   "metadata": {},
   "source": [
    "This notebook aim to provide a tutorial on how to run TRACK using python as well as on the hpcc. \n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "1. Create custom sclinac.dat inputs and track.dat inputs to be able to run any TRACK lattice file.\n",
    "\n",
    "2. Be able to run TRACK on n cores on Windows or Linux such as the MSU hpcc.\n",
    "\n",
    "3. Finally be able to save all the data as an .hdft5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f480c",
   "metadata": {},
   "source": [
    "## 1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37809c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading runTrack...\n",
      "Loading config...\n",
      "Loading utils...\n",
      "Loading hdf5Track...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'src')  # Change this to location of PyTrack src\n",
    "from runTrack import *\n",
    "from config import Config\n",
    "from utils import *\n",
    "from hdf5Track import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb393bd9",
   "metadata": {},
   "source": [
    "file path guide\n",
    "\n",
    "   /   = Root directory\n",
    "   \n",
    "   .   = This location\n",
    "   \n",
    "   ..  = Up a directory\n",
    "   \n",
    "   ./  = Current directory\n",
    "   \n",
    "   ../ = Parent of current directory\n",
    "   \n",
    "   ../../ = Two directories backwards\n",
    "\n",
    "'D:' tells which driver you are in for absolute directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf9bb1",
   "metadata": {},
   "source": [
    "## Check that src/runTrack.py works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f1b444",
   "metadata": {},
   "source": [
    "Go to unit_testing folder and start runing the unit testing. Make sure that works.\n",
    "\n",
    "If you are using Linux, you will need to make sure you have a track executable complied for your linux hardware. A version of track that was complied on MSU HPCC is included. If that doesn't work, contact Kei Fukushima for the track biuld. Then put the complied track into the **TRACK folder** and the **parentTRACK** folder. Make sure you give it permission to execute. This can be done using the cmd \"chmod 750 track\" or \"chmod -x track\"\n",
    "\n",
    "Can contact Kei Fukushima for a Linus version of TRACK: \n",
    "fukushim@frib.msu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db681a45",
   "metadata": {},
   "source": [
    "## 2. Setting up config.py\n",
    "\n",
    "You need to make sure you have the correct track executable on your machine. The following code should all run correctly if your paths are set correctly and you have the track executable in trackFiles.\n",
    "\n",
    "Make sure paths are set correctly. Change the paths in src/config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ead8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = Config()  # This is how you will access the config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "469c252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trackdir = cf.TRACK_DIRECTORY\n",
    "trackexe = cf.TRACK_EXE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a2c1f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CompletedProcess(args='C:\\\\Users\\\\trana\\\\Desktop\\\\TRACK_Development\\\\TRACK_reinforcement_learning\\\\TRACK\\\\TRACKv39C.exe', returncode=0),\n",
       " 0.5815575122833252)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runTrack(trackexe, trackdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45898a4b",
   "metadata": {},
   "source": [
    "If TRACK works, then the path were set correctly. The return code should be 0 and the exectution time, the number at the end, should be around .5 secs. Next we will set up the track.dat, the sclinac.dat, and the hdf5 configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17de01",
   "metadata": {},
   "source": [
    "### 2.1 Set up track.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b71ede",
   "metadata": {},
   "source": [
    "To set up the default parameters of track.dat, you just need to change the corresponding parameters of the TRACK_DAT dictionary. Make sure the paths are set correctly. Also some numbers might need to be written as a string, for example, if it has letters in them to indicate decimal places. Also make sure to download the following Field folder or if you know what you are going, create you own field folder. https://michiganstate-my.sharepoint.com/:f:/g/personal/tranant2_msu_edu/EkNo130XJrlBrLdz6ZS-HIYBE6peozQ279coF6U-dMDyXw?e=pJ55jt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c8a3f1",
   "metadata": {},
   "source": [
    "### 2.2 Set up sclinac.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e0853",
   "metadata": {},
   "source": [
    "To set up the defult sclinac.dat file, make a list of lists where each list correspond to each beam element and their settings. This directly writes each element in the sclinac file writing a new line for each list and putting spaces between each elements. Don't put the '0 stop' element at the end as the wrapper does that automattically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b327083",
   "metadata": {},
   "source": [
    "### 2.3 Set up HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbf4e70",
   "metadata": {},
   "source": [
    "This is going to be a bit complicated, but this allows you to specify exactly what data to save and how much to save.\n",
    "You will need to specify a couple of things in the hdf5 dictionary.\n",
    "1. 00_start - the initial settings in track.dat you want to save\n",
    "2. location - the beam elements you want to save\n",
    "3. inputs - the settings on the specific beam element you want to save\n",
    "4. outputs - the output from beam.out you want to save\n",
    "5. size - the total number of simulations you want to save\n",
    "6. index - index telling which sclinac element to get distro from. index=3 mean 3 elements up to element 3 will be recorded\n",
    "\n",
    "**00_start**\n",
    "This is a list of all the different track.dat settings you want to save. Make an item in the list is included in one of the veriables in track.dat. This will be all contained in a folder called 00_start\n",
    "\n",
    "**location**\n",
    "This is a list of all the different beam elements. This creates top level folders wih the name of each beam element. Should have the naming convention of '##_XXXX'.\n",
    "\n",
    "**Setting up inputs**\n",
    "This is a dictionary which gives the input of a beam element at different locations. The tags are the different locations. Given a tag, we get a list of tuples. Itss two element long where the first element gives the name of the dataset, and the second element gives the index number for where the variable is. For example:  [1, 'drift', 42.7, 3.0, 3.0] is the line that describes a drift space. If we want to save index 2, which is the length, then we would add the tuple ('length', 2) in the list. Example: 'inputs': {'00_drift':[('length',2),('xrap',3),('yrap',4)],'01_drift':[],'02_drift':[]},\n",
    "\n",
    "**Setting up outputs**\n",
    "This is also a dictionary which saves the output at the location of a beam element. The tags are the different locations. Given a tag, we get a list of tupes, but this time, the first index represent the varable being save and the second tells how many particles are being save. Because of how TRACK was programed, this is n + 1 for the reference particles.\n",
    "\n",
    "**size**\n",
    "This is the number of simulations we want to make\n",
    "\n",
    "**index**\n",
    "This is the index number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc948b",
   "metadata": {},
   "source": [
    "## 3. Run a single TRACK instance and save the data in an hdf5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db85b6",
   "metadata": {},
   "source": [
    "A run function does the following in order.\n",
    "\n",
    "1. update the track config file with the new settings spcify in run_config['track']\n",
    "2. update sclinac config file with the new settings spcify in run_config['sclinac']\n",
    "3. update track.dat with track config file using make_track\n",
    "4. save track inputs using save_inputs\n",
    "5. update sclinac.dat with sclinac config file\n",
    "6. save outputs of sclinac.dat at corrsponding locations (this is where the code runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12078c",
   "metadata": {},
   "source": [
    "Before you run everything, copy content of parent folder into child folder using copy2folder. This is where TRACK will run to keep things organized. \n",
    "\n",
    "Next you want to set up the hdf5 files using makehdf5()\n",
    "\n",
    "Next you want to set up the run_config dictionary. Notice you will need to input changes in both sclinac and inputs if you do any. The run_config is the main file you will want to change in order to update each track simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb66448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "cf = Config()\n",
    "hdf5_path = './test0.hdf5'\n",
    "hdf5_config = cf.HDF5\n",
    "\n",
    "copy2folder(cf.PARENT_TRACK,cf.CHILD_TRACK,'test')  # create folder\n",
    "sim_folder = str(cf.CHILD_TRACK)+'/test'\n",
    "makehdf5(hdf5_path, hdf5_config)  # make hdf5 file\n",
    "\n",
    "# This dictionary contains the variables you want to change.\n",
    "# Make sure the run_config matches the input and output hdf5 configs\n",
    "run_config = {'track': {'epsnx':'0.12d0',\n",
    "                        'alfax':'1.00d0',\n",
    "                        'betax':'100.0d0',\n",
    "                        'epsny':'0.12d0',\n",
    "                        'alfay':'1.00d0',\n",
    "                        'betay':'100.0d0'},\n",
    "             'sclinac': [[4,2,-6],[5,2,6],[7,2,-6],[8,2,6],[10,2,-6],[11,2,6]],\n",
    "             'inputs':{'05_eq3d':{'voltage':-6},  # first index in name, second index is value\n",
    "                       '06_eq3d':{'voltage':6},\n",
    "                      '08_eq3d':{'voltage':-6},\n",
    "                      '09_eq3d':{'voltage':6},\n",
    "                      '11_eq3d':{'voltage':-6},\n",
    "                      '12_eq3d':{'voltage':6}}\n",
    "             }\n",
    "\n",
    "run(run_config, sim_folder, hdf5_path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This is mainly how each run is configered. The run_config dictionary is changed and then\n",
    "#  A run is started. Here is the case for 6 different runs\n",
    "sclinac_q1 = [-6,-5,-4,-3,-2,-1]\n",
    "for i in range(len(sclinac_q1)):\n",
    "    run_config['sclinac'][0][2] = sclinac_q1[i]\n",
    "    run_config['inputs']['05_eq3d']['voltage'] = sclinac_q1[i]\n",
    "    run(run_config, sim_folder, hdf5_path, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f25bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with h5py.File('./test0.hdf5', \"a\") as database:\n",
    "    print(database[f'05_eq3d'].keys())\n",
    "    print(np.array(database['05_eq3d/inputs/voltage'][:6]))\n",
    "    print(np.array(database['12_eq3d/inputs/voltage'][:6]))\n",
    "    print(np.array(database['06_eq3d/outputs/x'][0]))\n",
    "    print(np.array(database['06_eq3d/outputs/x'][7]))\n",
    "    print(np.array(database['12_eq3d/outputs/y'][0]))\n",
    "    print(np.array(database['12_eq3d/outputs/y'][7]))\n",
    "    print(np.array(database['00_start/outputs/y'][0]))\n",
    "    print(np.array(database['00_start/outputs/y'][7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc0a98",
   "metadata": {},
   "source": [
    "The results should be something like this. Zeros where there should be zeros and numbers where there should be numbers.\n",
    "\n",
    "<KeysViewHDF5 ['inputs', 'outputs']>\n",
    "\n",
    "[-6. -5. -4. -3. -2. -1.]\n",
    "\n",
    "[6. 6. 6. 6. 6. 6.]\n",
    "\n",
    "[ 4.3201604e-07 -1.0199331e+00 -7.6650989e-01 ...  8.0134898e-02\n",
    " -2.4582863e-01 -9.5469528e-01]\n",
    " \n",
    "[0. 0. 0. ... 0. 0. 0.]\n",
    "\n",
    "[-8.7229802e-07  7.7979213e-01 -9.5388597e-01 ... -4.1215295e-01\n",
    " -5.0551176e-01 -1.0993438e+00]\n",
    " \n",
    "[0. 0. 0. ... 0. 0. 0.]\n",
    "\n",
    "[ 0.         -0.52694255  0.51611817 ...  0.7277629   0.7714185\n",
    "  0.49240583]\n",
    "  \n",
    "[0. 0. 0. ... 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hdf5 file\n",
    "if os.path.exists(hdf5_path):\n",
    "    os.remove(hdf5_path)\n",
    "# remove test folder\n",
    "rmfolder(cf.CHILD_TRACK,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896ffb6",
   "metadata": {},
   "source": [
    "# 4. Run track on multiple cores\n",
    "\n",
    "We will now do the last test, running TRACK on mulitple cores and saving all the data in an hdf5 file. How this code works is that each core will get its own child folder from the parent folder to run, then it will save in its own hdf5 file. First, I need to make a function which creates the hdf5 files, and iteratively runs a simulation with different configerations. Call this datagen. This will have to be customized for different configurations of inputs as this tells where each input goes. Start by testing it out here then copy datagen into the hdf5Track.py file in order for the next section to work. The next section cannot use the function define in a notebook. It has to be define in a separate .py file. Then commet out the datagen in the notebook and run everything again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def datagen(inputs, child, hdf5):\n",
    "#     \"\"\"\n",
    "#     inputs:\n",
    "#     inputs - vectors of different inputs. Must all have the same dimensions\n",
    "#     child - folder where simulation will run\n",
    "#     hdf5 - hdf5 file. Where all the data is stored\n",
    "#     \"\"\"\n",
    "#     # This dictionary contains the variables you want to change.\n",
    "#     # Make sure the run_config matches the input and output hdf5 configs\n",
    "#     # At the moment, you have to specify voltages for both the sclinac and inputs.\n",
    "#     for i in range(len(inputs)):\n",
    "#         v1 = inputs['v1']\n",
    "#         v2 = inputs['v2']\n",
    "#         v3 = inputs['v3']\n",
    "#         v4 = inputs['v4']\n",
    "#         v5 = inputs['v5']\n",
    "#         v6 = inputs['v6']\n",
    "#         run_config = {'track': {'epsnx':'0.12d0',\n",
    "#                                 'alfax':'1.00d0',\n",
    "#                                 'betax':'100.0d0',\n",
    "#                                 'epsny':'0.12d0',\n",
    "#                                 'alfay':'1.00d0',\n",
    "#                                 'betay':'100.0d0'},\n",
    "#                      'sclinac': [[4,2,v1[i]],[5,2,v2[i]],[7,2,v3[i]],[8,2,v4[i]],[10,2,v5[i]],[11,2,v6[i]]],\n",
    "#                      'inputs':{'05_eq3d':{'voltage':v1[i]},  # first index in name, second index is value\n",
    "#                                '06_eq3d':{'voltage':v2[i]},\n",
    "#                               '08_eq3d':{'voltage':v3[i]},\n",
    "#                               '09_eq3d':{'voltage':v4[i]},\n",
    "#                               '11_eq3d':{'voltage':v5[i]},\n",
    "#                               '12_eq3d':{'voltage':v6[i]}}\n",
    "#                      }\n",
    "#         run(run_config, child, hdf5, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "cf = Config()\n",
    "hdf5_dir = cf.HDF5_DIR\n",
    "hdf5_path = f'{hdf5_dir}/test.hdf5'\n",
    "hdf5_config = cf.HDF5\n",
    "\n",
    "copy2folder(cf.PARENT_TRACK,cf.CHILD_TRACK,'test')  # create folder\n",
    "sim_folder = str(cf.CHILD_TRACK)+'/test'\n",
    "makehdf5(hdf5_path, hdf5_config)  # make hdf5 file\n",
    "\n",
    "n = 10\n",
    "vv1 = np.random.rand(n)*8\n",
    "vv2 = -np.random.rand(n)*8\n",
    "vv3 = np.random.rand(n)*8\n",
    "vv4 = -np.random.rand(n)*8\n",
    "vv5 = np.random.rand(n)*8\n",
    "vv6 = -np.random.rand(n)*8\n",
    "inputs = {'v1':vv1,\n",
    "         'v2':vv2,\n",
    "         'v3':vv3,\n",
    "         'v4':vv4,\n",
    "         'v5':vv5,\n",
    "         'v6':vv6,}\n",
    "datagen(inputs, sim_folder, hdf5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab04e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_path, \"a\") as database:\n",
    "    print(database[f'05_eq3d'].keys())\n",
    "    print(np.array(database['05_eq3d/inputs/voltage'][:10]))\n",
    "    print(np.array(database['12_eq3d/inputs/voltage'][:10]))\n",
    "    print(np.array(database['06_eq3d/outputs/x'][0]))\n",
    "    print(np.array(database['06_eq3d/outputs/x'][1]))\n",
    "    print(np.array(database['06_eq3d/outputs/x'][2]))\n",
    "    print(np.array(database['06_eq3d/outputs/x'][6]))\n",
    "    print(np.array(database['06_eq3d/outputs/x'][7]))\n",
    "    print(np.array(database['06_eq3d/outputs/x'][10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025212c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hdf5 file\n",
    "if os.path.exists(hdf5_path):\n",
    "    os.remove(hdf5_path)\n",
    "# remove test folder\n",
    "rmfolder(cf.CHILD_TRACK,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5752964",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "n = 160  #number of simulations\n",
    "vv1 = np.random.rand(n)*8\n",
    "vv2 = -np.random.rand(n)*8\n",
    "vv3 = np.random.rand(n)*8\n",
    "vv4 = -np.random.rand(n)*8\n",
    "vv5 = np.random.rand(n)*8\n",
    "vv6 = -np.random.rand(n)*8\n",
    "distro = np.random.choice(np.array([0,1,3,4,5,6,7,8,9]),n)\n",
    "\n",
    "n=4  # Change this to get different numbers of cores\n",
    "cf = Config()\n",
    "parent_dir = cf.PARENT_TRACK\n",
    "child_dir = cf.CHILD_TRACK\n",
    "hdf5_dir =  cf.HDF5_DIR\n",
    "hdf5_config = cf.HDF5\n",
    "\n",
    "for file in os.listdir(child_dir):  # Removes all files in child_dir. If don't do this, then when run n=32 then n=4, it will still run all 32 first\n",
    "    shutil.rmtree(str(child_dir)+\"/\"+str(file))\n",
    "    \n",
    "sim_folder = [None]*n\n",
    "for i in range(n):  # creates the folders\n",
    "    copy2folder(parent_dir, child_dir, str(i))  # create the child folder names after numbers\n",
    "    sim_folder[i] = str(child_dir)+'/' + str(i)  # create a list of path to the child folders\n",
    "    makehdf5(f'{hdf5_dir}/{i}.hdf5', hdf5_config)  # make hdf5 file named after numbers '0','1',...'n'\n",
    "    \n",
    "inputs = [None]*n    \n",
    "nsim = len(vv1)\n",
    "vsize = int(nsim/n)\n",
    "for i in range(n):\n",
    "    inputs[i] = {'v1':vv1[vsize*i: vsize*(i+1)],  # split voltages evening among the cores\n",
    "         'v2':vv2[vsize*i: vsize*(i+1)],\n",
    "         'v3':vv3[vsize*i: vsize*(i+1)],\n",
    "         'v4':vv4[vsize*i: vsize*(i+1)],\n",
    "         'v5':vv5[vsize*i: vsize*(i+1)],\n",
    "         'v6':vv6[vsize*i: vsize*(i+1)]}\n",
    "    \n",
    "if (inputs[0]['v1'].shape[0] % n != 0):\n",
    "    print(\"Input shape not divisiable evenly by n cores!\")\n",
    "    print(str(voltages.shape[0])+\"%\"+str(n) +\"=\"+str(voltages.shape[0] % n ) )\n",
    "\n",
    "all_index=np.arange(n)\n",
    "items = [(inputs[i], sim_folder[i], f'{hdf5_dir}/{i}.hdf5') for i in all_index]\n",
    "\n",
    "with Pool(processes=n) as pool:\n",
    "    pool.starmap(datagen, items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343ac30",
   "metadata": {},
   "source": [
    "Note for future works. Pool.starmap requres a function in order to distribute it to all the cores. You can't have it define in this notebook. YOu have to have it define in a separate .py file such as hdf5TRACK.py in this case because this enable the computer to distribute the function out to different cores; I would even say you need to do this to all function that can potentilaly be distributed just to make sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f69f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
